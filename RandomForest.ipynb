{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72ca0d11",
   "metadata": {},
   "source": [
    "### Decision tree code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3afbbbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47d4f254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tqdm\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "292b71cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "class Node:\n",
    "    def __init__(self,feature_index = None, threshold = None, left = None, right = None,*,value = None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right =right\n",
    "        self.value = value\n",
    "    \n",
    "    #to check whether it is leaf node or not\n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "    \n",
    "    \n",
    "# create a decision tree\n",
    "class Decision_tree:\n",
    "    def __init__(self,min_samples_split =10, max_depth = 100, n_features= None):\n",
    "        \n",
    "        self.min_samples_split = min_samples_split          #specifies the minimum number of samples required \n",
    "                                                                   #to split an internal node into two branches.\n",
    "        self.max_depth = max_depth                          # to intialize the max depth to how long the tree can grow\n",
    "        self.n_features = n_features                        # to declare the no of features needed\n",
    "        self.root = None                                    # to acces the root of the tree\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        \n",
    "        #this line ensures that the n_features shouldn't exceed the X.shape \n",
    "        self.n_features= X.shape[1] if not self.n_features else min(X.shape[1],self.n_features)\n",
    "        \n",
    "        #helper function\n",
    "        self.root = self._grow_tree(X,y)\n",
    "        \n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def _grow_tree(self,X,y,depth = 0):\n",
    "        n_samples, n_feats = X.shape\n",
    "        n_labels = len(np.unique(y))        #gives the n labels\n",
    "        \n",
    "        #check the stopping criteria (provide a criteria)\n",
    "        \n",
    "        \n",
    "        if (depth>=self.max_depth or n_labels ==1 or n_samples< self.min_samples_split):  #if condtion meets leaf value will be return\n",
    "            leaf_value = self._most_common_label(y)  \n",
    "            return Node(value = leaf_value)                                                #this will return the leaf value\n",
    "        \n",
    "        \n",
    "        feat_indexs = np.random.choice(n_feats,self.n_features, replace= False) #to get the unique features while updating the group\n",
    "        \n",
    "        #find the best split\n",
    "        best_feature,best_threshold = self._best_split(X,y,feat_indexs)\n",
    "        \n",
    "        #create child nodes by(accesing the _grow_tree function)\n",
    "        left_idxs,right_idxs = self._split(X[:,best_feature], best_threshold)\n",
    "        left =  self._grow_tree(X[left_idxs,:],y[left_idxs], depth+1)\n",
    "        right = self._grow_tree(X[right_idxs,:],y[right_idxs],depth+1)\n",
    "        return Node(best_feature,best_threshold,left,right)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def _best_split(self,X,y,feat_indexs):\n",
    "        best_gain = -1                               # Initialize best gain to a very low value\n",
    "        split_index = None                           # To store the index of the best feature for the split\n",
    "        split_threshold = None                       # To store the best threshold for the split\n",
    "        \n",
    "        \n",
    "        for feat_index in feat_indexs:                # Iterate over all feature indices\n",
    "            X_column = X[:,feat_index]                # Extract the values of the current feature (column)\n",
    "            thresholds= np.unique(X_column)           # Get unique values (thresholds) in the current feature\n",
    "            \n",
    "            for thr in thresholds:                              # Iterate over all unique thresholds (possible split points)\n",
    "                #to calculate the inforamtion gain\n",
    "                gain  = self._information_gain(y,X_column,thr)        # Calculate the information gain for the current feature and threshold\n",
    "                                                                  # information gain using helper funtion\n",
    "                \n",
    "             # if the gain is better than the best found till now, upadte the best_split parameters\n",
    "        \n",
    "                if gain >best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_index = feat_index\n",
    "                    split_threshold = thr\n",
    "                    \n",
    "        return split_index,split_threshold\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ## INFORMATION GAIN USING ENTROPY\n",
    "        \n",
    "        \n",
    "#     def _information_gain(self,y,X_column,threshold):\n",
    "        \n",
    "        \n",
    "#         # parent entropy\n",
    "#         parent_entropy = self._entropy(y)\n",
    "                                    \n",
    "#         #create children\n",
    "#         left_idxs,right_idxs= self._split(X_column,threshold)\n",
    "        \n",
    "#         if len(left_idxs)==0 or len(right_idxs) ==0:\n",
    "#             return 0\n",
    "        \n",
    "#         #calculate the weighted average of the entropy of the children\n",
    "#         n = len(y)\n",
    "#         n_left,n_right = len(left_idxs), len(right_idxs)\n",
    "#         e_left,e_right = self._entropy(y[left_idxs]),self._entropy(y[right_idxs])\n",
    "        \n",
    "#         child_entropy = (n_left/n) * e_left / (n_right/n) * e_right   \n",
    "        \n",
    "#         #Calculate the information gain\n",
    "        \n",
    "#         information_gain = parent_entropy - child_entropy\n",
    "#         return information_gain\n",
    "        \n",
    "     \n",
    "        \n",
    "        ## INFORMATION GAIN USING GINI INDEX\n",
    "        \n",
    "    def _information_gain(self, y, X_column, threshold):\n",
    "        # Use Gini impurity \n",
    "        parent_gini = 1 - np.sum((np.bincount(y) / len(y)) ** 2)\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "        left_gini = 1 - np.sum((np.bincount(y[left_idxs]) / len(left_idxs)) ** 2)\n",
    "        right_gini = 1 - np.sum((np.bincount(y[right_idxs]) / len(right_idxs)) ** 2)\n",
    "        child_gini = (len(left_idxs) / len(y)) * left_gini + (len(right_idxs) / len(y)) * right_gini\n",
    "        return parent_gini - child_gini\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def _split(self,X_column,split_threshold):\n",
    "        \n",
    "        # Get indices of the data points that belong to the left and right node (<= threshold,>threshold)\n",
    "        \n",
    "        left_idxs = np.argwhere(X_column <= split_threshold).flatten()   #argwhere is used to convert the output from 2d array to 1d array\n",
    "        right_idxs =np.argwhere(X_column > split_threshold).flatten()\n",
    "        \n",
    "        return left_idxs,right_idxs\n",
    "        \n",
    "        \n",
    "\n",
    "    # Entropy formula =  -summization(p(x))*log2(p(x))\n",
    "    \n",
    "    def _entropy(self,y):            \n",
    "        hist  = np.bincount(y)                   #it will create a histogram of number(telling the no. of times a digit occurs)\n",
    "        ps  = hist/len(y)                        # formula for p(x)  = number of times class appeared / number of total nodes\n",
    "        \n",
    "        return -np.sum([p* np.log(p) for p in ps if p>0])   #formula for entropy with a condition\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _most_common_label(self,y):                 # helper function to get the most common leaf value\n",
    "        common = Counter(y)\n",
    "        value = common.most_common(1)[0][0]\n",
    "        return value\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature_index] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f237d9",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "285c8ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialzing function of  the RandomForest model\n",
    "class RandomForest:\n",
    "    def __init__(self,n_trees = 10,max_depth= 10, min_samples_split =2, n_features = None):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.n_features = n_features\n",
    "        self.trees = []\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_trees):\n",
    "            tree = Decision_tree(max_depth = self.max_depth,\n",
    "                                min_samples_split = self.min_samples_split,\n",
    "                                n_features = self.n_features)\n",
    "            X_sample,y_sample = self._bootstrap_feature(X,y)\n",
    "            tree.fit(X_sample,y_sample)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "    # here we use most common function that will return the most common occured values using the counter\n",
    "    \n",
    "    def _most_common_label(self,y):                            # helper function to get the most common leaf value\n",
    "        \n",
    "        common = Counter(y)\n",
    "        value = common.most_common(1)[0][0]\n",
    "        return value\n",
    "    \n",
    "    def _bootstrap_feature(self,X,y):\n",
    "        n_samples = X.shape[0]\n",
    "        idxs = np.random.choice(n_samples,n_samples,replace = True) # storing the random shuffled and repeated data  into idxs #some samples will be repeated while other may be left out \n",
    "        return X[idxs],y[idxs]                                       #returning it in X and y\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def Predict(self,X):\n",
    "        predictions  = np.array([tree.predict(X)for tree in self.trees])\n",
    "        tree_preds = np.swapaxes(predictions,0,1)                      \n",
    "        predictions = np.array([self._most_common_label(pred) for pred in tree_preds])\n",
    "        return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71963b0a",
   "metadata": {},
   "source": [
    "#### training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71ca660f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n\u001b[0;32m     15\u001b[0m clf \u001b[38;5;241m=\u001b[39m RandomForest()\n\u001b[1;32m---> 16\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_train,y_train)\n\u001b[0;32m     17\u001b[0m predcitions \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mPredict(X_test)\n\u001b[0;32m     20\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy(y_test,predcitions)\n",
      "Cell \u001b[1;32mIn[34], line 17\u001b[0m, in \u001b[0;36mRandomForest.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     13\u001b[0m tree \u001b[38;5;241m=\u001b[39m Decision_tree(max_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth,\n\u001b[0;32m     14\u001b[0m                     min_samples_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_split,\n\u001b[0;32m     15\u001b[0m                     n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features)\n\u001b[0;32m     16\u001b[0m X_sample,y_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bootstrap_feature(X,y)\n\u001b[1;32m---> 17\u001b[0m tree\u001b[38;5;241m.\u001b[39mfit(X_sample,y_sample)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrees\u001b[38;5;241m.\u001b[39mappend(tree)\n",
      "Cell \u001b[1;32mIn[40], line 37\u001b[0m, in \u001b[0;36mDecision_tree.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features\u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m#helper function\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grow_tree(X,y)\n",
      "Cell \u001b[1;32mIn[40], line 67\u001b[0m, in \u001b[0;36mDecision_tree._grow_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m#create child nodes by(accesing the _grow_tree function)\u001b[39;00m\n\u001b[0;32m     66\u001b[0m left_idxs,right_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(X[:,best_feature], best_threshold)\n\u001b[1;32m---> 67\u001b[0m left \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grow_tree(X[left_idxs,:],y[left_idxs], depth\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     68\u001b[0m right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grow_tree(X[right_idxs,:],y[right_idxs],depth\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Node(best_feature,best_threshold,left,right)\n",
      "Cell \u001b[1;32mIn[40], line 67\u001b[0m, in \u001b[0;36mDecision_tree._grow_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m#create child nodes by(accesing the _grow_tree function)\u001b[39;00m\n\u001b[0;32m     66\u001b[0m left_idxs,right_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(X[:,best_feature], best_threshold)\n\u001b[1;32m---> 67\u001b[0m left \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grow_tree(X[left_idxs,:],y[left_idxs], depth\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     68\u001b[0m right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grow_tree(X[right_idxs,:],y[right_idxs],depth\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Node(best_feature,best_threshold,left,right)\n",
      "Cell \u001b[1;32mIn[40], line 67\u001b[0m, in \u001b[0;36mDecision_tree._grow_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m#create child nodes by(accesing the _grow_tree function)\u001b[39;00m\n\u001b[0;32m     66\u001b[0m left_idxs,right_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(X[:,best_feature], best_threshold)\n\u001b[1;32m---> 67\u001b[0m left \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grow_tree(X[left_idxs,:],y[left_idxs], depth\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     68\u001b[0m right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grow_tree(X[right_idxs,:],y[right_idxs],depth\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Node(best_feature,best_threshold,left,right)\n",
      "Cell \u001b[1;32mIn[40], line 68\u001b[0m, in \u001b[0;36mDecision_tree._grow_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n\u001b[0;32m     66\u001b[0m left_idxs,right_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(X[:,best_feature], best_threshold)\n\u001b[0;32m     67\u001b[0m left \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grow_tree(X[left_idxs,:],y[left_idxs], depth\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grow_tree(X[right_idxs,:],y[right_idxs],depth\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Node(best_feature,best_threshold,left,right)\n",
      "Cell \u001b[1;32mIn[40], line 63\u001b[0m, in \u001b[0;36mDecision_tree._grow_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n\u001b[0;32m     60\u001b[0m feat_indexs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(n_feats,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features, replace\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m#to get the unique features while updating the group\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m#find the best split\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m best_feature,best_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_best_split(X,y,feat_indexs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m#create child nodes by(accesing the _grow_tree function)\u001b[39;00m\n\u001b[0;32m     66\u001b[0m left_idxs,right_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(X[:,best_feature], best_threshold)\n",
      "Cell \u001b[1;32mIn[40], line 88\u001b[0m, in \u001b[0;36mDecision_tree._best_split\u001b[1;34m(self, X, y, feat_indexs)\u001b[0m\n\u001b[0;32m     84\u001b[0m thresholds\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(X_column)           \u001b[38;5;66;03m# Get unique values (thresholds) in the current feature\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m thr \u001b[38;5;129;01min\u001b[39;00m thresholds:                              \u001b[38;5;66;03m# Iterate over all unique thresholds (possible split points)\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m#to calculate the inforamtion gain\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m     gain  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_information_gain(y,X_column,thr)        \u001b[38;5;66;03m# Calculate the information gain for the current feature and threshold\u001b[39;00m\n\u001b[0;32m     89\u001b[0m                                                       \u001b[38;5;66;03m# information gain using helper funtion\u001b[39;00m\n\u001b[0;32m     90\u001b[0m     \n\u001b[0;32m     91\u001b[0m  \u001b[38;5;66;03m# if the gain is better than the best found till now, upadte the best_split parameters\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gain \u001b[38;5;241m>\u001b[39mbest_gain:\n",
      "Cell \u001b[1;32mIn[40], line 137\u001b[0m, in \u001b[0;36mDecision_tree._information_gain\u001b[1;34m(self, y, X_column, threshold)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_information_gain\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, X_column, threshold):\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# Use Gini impurity \u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     parent_gini \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39msum((np\u001b[38;5;241m.\u001b[39mbincount(y) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    138\u001b[0m     left_idxs, right_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(X_column, threshold)\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(left_idxs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(right_idxs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:2172\u001b[0m, in \u001b[0;36m_sum_dispatcher\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2103\u001b[0m \u001b[38;5;124;03m    Clip (limit) the values in an array.\u001b[39;00m\n\u001b[0;32m   2104\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2167\u001b[0m \n\u001b[0;32m   2168\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m'\u001b[39m, a_min, a_max, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 2172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum_dispatcher\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2173\u001b[0m                     initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[0;32m   2177\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_sum_dispatcher)\n\u001b[0;32m   2178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m   2179\u001b[0m         initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data  = datasets.load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=45)\n",
    "\n",
    "\n",
    "def accuracy(y_true,y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred)/len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "clf = RandomForest()\n",
    "clf.fit(X_train,y_train)\n",
    "predcitions = clf.Predict(X_test)\n",
    "\n",
    "\n",
    "acc = accuracy(y_test,predcitions)\n",
    "print(\"the accuacy of the RandomForest model = \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab369496",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
